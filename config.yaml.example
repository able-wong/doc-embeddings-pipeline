# Document Ingestion Pipeline Configuration Template
# 
# Copy this file to config.yaml and configure for your setup:
# cp config.yaml.example config.yaml
#
# This file contains examples for all supported providers and configurations.
# Uncomment and configure the sections you need.

# ============================================================================
# DOCUMENT PROCESSING SETTINGS
# ============================================================================
documents:
  folder_path: "./documents"  # Path to folder containing documents to process
  supported_extensions: [".txt", ".docx", ".pdf", ".md", ".html", ".json"]  # File types to process
  chunk_size: 1000      # Maximum characters per text chunk
  chunk_overlap: 200    # Characters to overlap between chunks (for context continuity)

# ============================================================================
# EMBEDDING CONFIGURATION
# ============================================================================
embedding:
  # PROVIDER SELECTION: Choose "ollama", "gemini", or "sentence_transformers"
  provider: "ollama"  # Options: "ollama", "gemini", "sentence_transformers"
  
  # OLLAMA SETTINGS (Local embedding server)
  # Use these settings when provider = "ollama"
  model: "nomic-embed-text"           # Recommended: 768 dimensions, 274MB
  # model: "mxbai-embed-large"        # Alternative: 1024 dimensions, 669MB (higher quality)
  base_url: "http://localhost:11434" # Change if Ollama runs on different host
  timeout: 60                         # Request timeout in seconds
  
  # GEMINI SETTINGS (Google Cloud API)
  # Use these settings when provider = "gemini"
  # Uncomment and configure when switching to Gemini:
  gemini:
    api_key: ""                       # REQUIRED: Your Gemini API key (or use GEMINI_API_KEY env var)
    model: "text-embedding-004"       # Google's latest embedding model (768 dimensions)
  
  # SENTENCE TRANSFORMERS SETTINGS (Local Python library)
  # Use these settings when provider = "sentence_transformers"
  # Uncomment and configure when switching to Sentence Transformers:
  sentence_transformers:
    model: "all-MiniLM-L6-v2"         # Lightweight model: 384 dimensions, ~90MB
    # model: "all-mpnet-base-v2"      # Alternative: Higher quality, 768 dimensions, ~420MB
    device: "cpu"                     # Options: "cpu", "cuda" (GPU), "mps" (Apple Silicon)

# ============================================================================
# EMBEDDING PROVIDER SETUP EXAMPLES
# ============================================================================
# 
# SETUP FOR OLLAMA (LOCAL):
# 1. Install: brew install ollama
# 2. Start service: ollama serve
# 3. Download model: ollama pull nomic-embed-text
# 4. Set provider: "ollama" (already set above)
# 5. Update base_url if Ollama runs on different host/port
#
# SETUP FOR GEMINI (CLOUD API):
# 1. Get API key from: https://ai.google.dev/
# 2. Set provider: "gemini" 
# 3. Set API key either:
#    a) In gemini.api_key above, OR
#    b) Environment variable: export GEMINI_API_KEY="your-key"
# 4. Update collection_name to avoid mixing embeddings from different providers
#
# SETUP FOR SENTENCE TRANSFORMERS (LOCAL PYTHON):
# 1. Install: pip install sentence-transformers
# 2. Set provider: "sentence_transformers"
# 3. Choose model: "all-MiniLM-L6-v2" (fast, 384D) or "all-mpnet-base-v2" (better, 768D)
# 4. Set device: "cpu" (default), "cuda" (NVIDIA GPU), or "mps" (Apple Silicon)
# 5. Update collection_name to avoid mixing embeddings from different providers
#
# IMPORTANT: Different providers produce different embedding dimensions!
# Clear your vector database when switching providers: python3 ingest.py clear-all

# ============================================================================
# LLM CONFIGURATION (for metadata extraction)
# ============================================================================
llm:
  # PROVIDER SELECTION: Choose "ollama" or "gemini"
  provider: "ollama"  # Options: "ollama", "gemini"
  
  # OLLAMA SETTINGS (Local LLM server)
  # Use these settings when provider = "ollama"
  model: "llama3.2"                   # Recommended: Fast and capable for metadata extraction
  # model: "llama3.2:3b"              # Alternative: Smaller, faster model
  # model: "qwen2.5:7b"               # Alternative: Good for structured data extraction
  base_url: "http://localhost:11434" # Change if Ollama runs on different host
  timeout: 120                        # Request timeout in seconds (LLM calls take longer)
  
  # GEMINI SETTINGS (Google Cloud API)
  # Use these settings when provider = "gemini"
  # Uncomment and configure when switching to Gemini:
  gemini:
    api_key: ""                       # REQUIRED: Your Gemini API key (or use GEMINI_API_KEY env var)
    model: "gemini-1.5-flash"         # Google's fast model for text processing
  
  # CONTENT PROCESSING SETTINGS
  content_max_chars: 8000             # Minimum content limit and fallback when auto-detection fails
  auto_detect_context_limit: true     # Automatically adjust based on model capabilities
  context_utilization: 0.25           # Percentage of context window to use (0.25 = 25%)
  
  # METADATA EXTRACTION SETTINGS
  max_retries: 3                      # Number of retries if LLM extraction fails
  metadata_extraction_prompt: |       # Customizable prompt for metadata extraction
    You are a metadata extraction assistant. Your task is to analyze the provided document and extract specific metadata fields.

    Return a valid JSON object with these exact fields:

    {{
      "author": "string or null",
      "title": "string", 
      "publication_date": "YYYY-MM-DD or null",
      "tags": ["topic1", "topic2", "topic3"]
    }}

    Guidelines:
    - Author: Look for "By [Name]", "Author: [Name]", "[Name] writes", or extract from URL paths like "/authors/name/" or "/john-doe/"
    - Title: Extract from content headers/titles, or clean filename (remove dates, convert dashes to spaces)
    - Date: Find "Published", "Posted", dates like "Jan 2025", "2025-01-20", "January 20, 2025", or YYYY-MM-DD in filename
    - Tags: Extract 3-7 relevant keywords/topics from the actual content
    - Return valid JSON only, no other text

    Document to analyze:
    SOURCE URL: {source_url}
    FILENAME: {filename}
    CONTENT: {content}

# ============================================================================
# LLM PROVIDER SETUP EXAMPLES
# ============================================================================
# 
# SETUP FOR OLLAMA (LOCAL):
# 1. Install: brew install ollama
# 2. Start service: ollama serve
# 3. Download model: ollama pull llama3.2
# 4. Set provider: "ollama" (already set above)
# 5. Update base_url if Ollama runs on different host/port
#
# SETUP FOR GEMINI (CLOUD API):
# 1. Get API key from: https://ai.google.dev/
# 2. Set provider: "gemini" 
# 3. Set API key either:
#    a) In gemini.api_key above, OR
#    b) Environment variable: export GEMINI_API_KEY="your-key"
#
# CUSTOMIZING METADATA EXTRACTION:
# - Modify metadata_extraction_prompt to change extraction behavior
# - Adjust max_retries for reliability vs speed tradeoff
# - The prompt supports {filename} and {content} placeholders
# - Note: filename is derived from source_url when needed
#
# CONTENT PROCESSING OPTIMIZATION:
# - content_max_chars: Minimum content limit (8k chars) and fallback when auto-detection fails
# - auto_detect_context_limit: Automatically detects model context window
# - context_utilization: Percentage of context window to use for metadata extraction
#   * 0.25 (25%): llama3.2: 128k tokens → ~128k chars, gemini-1.5-flash: 1M tokens → ~1M chars  
#   * 0.5 (50%): More aggressive utilization for large documents
#   * 0.1 (10%): Conservative utilization for cost savings
#   * Actual limit = max(content_max_chars, detected_context * context_utilization)

# ============================================================================
# VECTOR DATABASE CONFIGURATION
# ============================================================================
vector_db:
  provider: "qdrant"              # Vector database type (currently only qdrant supported)
  
  # LOCAL QDRANT SETTINGS (Default - for self-hosted instance)
  host: "localhost"               # Qdrant server host
  port: 6333                      # Qdrant server port
  
  # QDRANT CLOUD SETTINGS (Takes precedence if url is provided)
  # Uncomment and configure for Qdrant Cloud:
  #url: "https://your-cluster-url.gcp.cloud.qdrant.io:6333"  # Your Qdrant Cloud URL
  #api_key: ""                   # Your Qdrant Cloud API key (or use QDRANT_API_KEY env var)
  
  # COLLECTION SETTINGS
  collection_name: "documents"    # Collection name for storing embeddings
  # collection_name: "documents_gemini"  # Use different collection for different providers
  distance_metric: "cosine"       # Similarity metric: "cosine", "euclidean", or "dot"

# ============================================================================
# VECTOR DATABASE SETUP EXAMPLES
# ============================================================================
#
# SETUP FOR LOCAL QDRANT (Docker):
# 1. Install Docker if not already installed
# 2. Run: docker run -d --name qdrant -p 6333:6333 -p 6334:6334 -v $(pwd)/qdrant_storage:/qdrant/storage:z qdrant/qdrant
# 3. Use host/port settings above (localhost:6333)
#
# SETUP FOR QDRANT CLOUD:
# 1. Sign up at: https://cloud.qdrant.io/
# 2. Create a cluster and get URL + API key
# 3. Uncomment url and api_key above
# 4. Set API key either:
#    a) In api_key above, OR
#    b) Environment variable: export QDRANT_API_KEY="your-key"

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
logging:
  level: "INFO"  # Log level: DEBUG (verbose), INFO (normal), WARNING, ERROR (minimal)

# ============================================================================
# COMMON CONFIGURATION COMBINATIONS
# ============================================================================
#
# 1. LOCAL DEVELOPMENT (Default above):
#    - Ollama + Local Qdrant
#    - provider: "ollama", host: "localhost"
#
# 2. PYTHON LOCAL + LOCAL VECTOR DB:
#    - provider: "sentence_transformers"
#    - No external dependencies, all runs locally
#    - Use host: "localhost" for Qdrant
#
# 3. CLOUD EMBEDDINGS + LOCAL VECTOR DB:
#    - provider: "gemini"
#    - Set GEMINI_API_KEY environment variable
#    - Use host: "localhost" for Qdrant
#
# 4. LOCAL EMBEDDINGS + CLOUD VECTOR DB:
#    - provider: "ollama" or "sentence_transformers"
#    - Uncomment url and api_key for Qdrant Cloud
#    - Set QDRANT_API_KEY environment variable
#
# 5. FULL CLOUD SETUP:
#    - provider: "gemini"
#    - Uncomment url and api_key for Qdrant Cloud
#    - Set both GEMINI_API_KEY and QDRANT_API_KEY environment variables
#
# Remember to use different collection_name when switching embedding providers!
# Dimensions: Ollama/Gemini=768, Sentence Transformers all-MiniLM-L6-v2=384